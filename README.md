
# rag-repro-baseline â€” Repro & benchmark dâ€™un pipeline RAG (FR/EN)

** Pipeline minimal et reproductible pour Retrieval-Augmented Generation (RAG) :
BM25 â†’ dense retriever (SBERT) â†’ fusion RRF â†’ (option) CrossEncoder reranking â†’ Ã©valuation P@k.
Projet personnel de Salma Makhlouf pour candidatures de thÃ¨se (NLP/IR, long & structured docs). **  

## ğŸ¯ Pourquoi ce mini-projet ?

Avoir une baseline propre et reproductible pour discuter RAG en contexte de thÃ¨se.

Mesurer lâ€™impact des choix de retrieval (BM25 vs dense vs RRF vs reranking CE).

PrÃ©parer des ablations (top-k, Î± de RRF, etc.) et une analyse dâ€™erreurs transfÃ©rable Ã  des jeux de donnÃ©es plus complexes (documents longs/structurÃ©s).

## ğŸ”§ Pipeline
[Corpus .txt] â”€â”€â–º BM25 (lexical)
               â””â”€â–º SBERT (dense)
                    â””â”€â–º RRF (fusion, Î± configurable)
                          â””â”€â–º (option) CrossEncoder rerank
                                 â””â”€â–º Ã‰val (Precision@k)

- Dense retriever : sentence-transformers/all-MiniLM-L6-v2
- RRF (Reciprocal Rank Fusion) : Î±=0.6 par dÃ©faut
- Reranking (option) : cross-encoder/ms-marco-MiniLM-L-6-v2
- Ã‰val : Precision@k (macro) + dÃ©tail par requÃªte

## ğŸ“‚ DonnÃ©es (mini dev-set fourni)

DATA/Corpus/ â€“ petits documents .txt (FR)
DATA/queries/dev.json â€“ requÃªtes + gold (doc_id attendus)
âš ï¸ Pour vos propres jeux de donnÃ©es, gardez 1â€“3 gold docs par requÃªte et documentez vos critÃ¨res dâ€™annotation.

## ğŸš€ Installation & exÃ©cution
Option A â€” GitHub Codespaces (recommandÃ©)

python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install "torch==2.3.1" --index-url https://download.pytorch.org/whl/cpu
pip install rank-bm25 sentence-transformers numpy pyyaml scikit-learn

#### ExÃ©cuter le pipeline (attention aux majuscules des dossiers) :
# 1) BM25
python src/index_bm25.py --config Configs/bm25.yaml \
  --queries DATA/queries/dev.json \
  --out Outputs/bm25_candidates.json

# 2) Dense retriever
python src/dense_retriever.py --config Configs/hybrid.yaml \
  --queries DATA/queries/dev.json \
  --out Outputs/dense_candidates.json

# 3) Fusion RRF + (option) CE rerank
python src/rerank_ce.py \
  --bm25 Outputs/bm25_candidates.json \
  --dense Outputs/dense_candidates.json \
  --queries DATA/queries/dev.json \
  --config Configs/hybrid.yaml \
  --out Outputs/hybrid_reranked.json

# 4) Ã‰valuation
python src/eval_patk.py \
  --run Outputs/hybrid_reranked.json \
  --gold DATA/queries/dev.json \
  --out Outputs/metrics.json

## ğŸ“ MÃ©triques

Precision@k (macro-moyenne sur les requÃªtes) :

$$
\mathrm{P@k}(q)=\frac{\#\{\text{docs pertinents dans les }k\text{ premiers}\}}{k}
\qquad
\mathrm{Macro}\text{-}\mathrm{P@k}
=\frac{1}{|Q|}\sum_{q\in Q}\mathrm{P@k}(q)
$$

OÃ¹ \(Q\) est lâ€™ensemble des requÃªtes; `k` est la coupure (par dÃ©faut \(k=10\)).

## âœ… RÃ©sultats (mini dev-set fourni)

Exemple obtenu sur le dev-set inclus (3 requÃªtes, 3 docs gold) :
Macro P@10 â‰ˆ 1.00 (attendu car corpus jouet et gold simple)

Fichiers gÃ©nÃ©rÃ©s :
Outputs/bm25_candidates.json
Outputs/dense_candidates.json
Outputs/hybrid_reranked.json
Outputs/metrics.json â† score final

Vos valeurs rÃ©elles dÃ©pendront du corpus, des requÃªtes et de la configuration.

## ğŸ§ª Ablations Ã  tester rapidement

Ã‰ditez Configs/hybrid.yaml :
bm25_topk, dense_topk, hybrid_topk
alpha (poids de la RRF)
(option) ajoutez des paramÃ¨tres chunk_size / overlap si vous chaÃ®nez ensuite la GÃ©nÃ©ration.
## Template de suivi (EXPERIMENTS.md)

| Exp | Config (k / Î± / CE?)                                   | **Macro P@10** | Notes              |
|:---:|:-------------------------------------------------------|:--------------:|--------------------|
| v1  | bm25=50, dense=200, Î±=0.6,<br>CE=off                   | 0.XX           | baseline RRF       |
| v2  | bm25=100, dense=200, Î±=0.6,<br>CE=on                   | 0.XX           | +CrossEncoder      |
| v3  | bm25=50, dense=200, Î±=0.3,<br>CE=on                    | 0.XX           | RRF moins agressif |


## ğŸ” Analyse dâ€™erreurs (exemple de structure)

Q2 â€“ â€œmÃ©dicament contre douleurs abdominales intestinâ€
Gold : doc2.txt (Colospa)
Top-1 : doc3.txt (RAG/LLM)
HypothÃ¨se : similaritÃ© lexicale faible, manque de synonymes (â€œantispasmodiqueâ€) â†’ amÃ©liorer vocabulaire, augmenter dense_topk, tester un modÃ¨le FR.

Q3 â€“ â€œqu est ce que RAG avec LLM et rerankingâ€
Gold : doc3.txt
ObservÃ© : correct en top-k, CE renforce la position
Note : CE utile pour formuler â€œquery, passageâ€ plus sÃ©mantique.

## ğŸ—‚ï¸ Structure du dÃ©pÃ´t
Configs/
  bm25.yaml         # corpus_dir, topk
  hybrid.yaml       # dense model, RRF (alpha), topk, CE
DATA/
  Corpus/           # .txt
  queries/
    dev.json        # queries + gold (doc_id)
Outputs/            # rÃ©sultats gÃ©nÃ©rÃ©s (.json)
src/
  index_bm25.py     # BM25
  dense_retriever.py# SBERT
  rerank_ce.py      # RRF + (option) CrossEncoder
  eval_patk.py      # Precision@k (macro + dÃ©tails)
Main.py             # orchestrateur (peut Ãªtre adaptÃ©)
requirements.txt
README.md

## ğŸ“Œ Points â€œrechercheâ€ mis en avant
Baseline reproductible (scripts + configs)
RRF et CrossEncoder reranking sÃ©parÃ©s pour ablations propres
P@k macro + notes dâ€™erreurs pour guider lâ€™amÃ©lioration
Corpus/queries FR (pertinent pour RAG long/structurÃ© en contexte francophone)

## Pour mettre Ã  jour la section RÃ©sultats

AprÃ¨s exÃ©cution :
cat Outputs/metrics.json
